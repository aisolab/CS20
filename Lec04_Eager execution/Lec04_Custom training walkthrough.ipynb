{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 20 : TensorFlow for Deep Learning Research\n",
    "## Lecture 04 : Eager execution\n",
    "### Custon training walkthrough\n",
    "Categorizing Iris flowers by species by using Tensorflow's eager execution.\n",
    "\n",
    "This guide uses these high-level TensorFlow concepts:\n",
    "\n",
    "* Enable an [eager execution](https://www.tensorflow.org/guide/eager?hl=ko) development environment,\n",
    "* Import data with the [Datasets API](https://www.tensorflow.org/guide/datasets?hl=ko)\n",
    "* Build models and layers with TensorFlow's [Keras API](https://keras.io/getting-started/sequential-model-guide/)  \n",
    "  \n",
    "  \n",
    "* Reference\n",
    "    + https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape = (4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, features, label):\n",
    "    score = model(features)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels = label, logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 10) dtype=float32, numpy=\n",
      "array([[ 0.06804562, -0.45806283, -0.11035424, -0.64274293,  0.18820602,\n",
      "         0.0355705 ,  0.23375762,  0.6061579 , -0.36892197, -0.5656868 ],\n",
      "       [-0.09489417, -0.28409988, -0.44565356,  0.11373091, -0.12645435,\n",
      "        -0.51101196,  0.5629221 , -0.38564408,  0.25186807,  0.19694126],\n",
      "       [ 0.12437344,  0.4322877 , -0.52797174, -0.2261037 , -0.62911326,\n",
      "        -0.07957786, -0.03850067, -0.24269658, -0.04691249, -0.10672408],\n",
      "       [ 0.6275412 ,  0.36151087, -0.57334185, -0.45194614, -0.43017197,\n",
      "         0.12567282, -0.2393085 ,  0.04838419,  0.52742684,  0.5202192 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[-0.44789726, -0.15868947, -0.11684254,  0.40309513, -0.12921724,\n",
      "         0.02773911,  0.07647151, -0.5429949 ,  0.3973015 , -0.1575292 ],\n",
      "       [-0.15320495,  0.33611304,  0.52707875, -0.46045673,  0.43035746,\n",
      "         0.01164854,  0.38064563,  0.22327399,  0.1757012 , -0.15881956],\n",
      "       [-0.5445693 , -0.15464652, -0.23653239,  0.5122181 , -0.32381445,\n",
      "        -0.28075787,  0.151672  ,  0.29994738, -0.08182597,  0.45518827],\n",
      "       [ 0.44842118,  0.33163166, -0.19526225,  0.05066091,  0.11708885,\n",
      "         0.42151403, -0.30990034,  0.3962226 , -0.33754542,  0.19665056],\n",
      "       [ 0.0958069 ,  0.46437883, -0.26469237, -0.06286967,  0.39041835,\n",
      "         0.12947518, -0.49272263,  0.45850468, -0.41122848,  0.03976643],\n",
      "       [-0.03450847,  0.2879356 , -0.3881131 ,  0.03022808, -0.3113752 ,\n",
      "        -0.21253008,  0.12392312, -0.25153008,  0.01663667, -0.53460145],\n",
      "       [ 0.08194286, -0.04415679, -0.01033914, -0.02042997,  0.4909873 ,\n",
      "         0.30873555,  0.23010409, -0.14592248,  0.13882196,  0.49277472],\n",
      "       [ 0.27612895, -0.13359454, -0.05620682, -0.10904479, -0.14941034,\n",
      "        -0.25980437, -0.06870234,  0.2930141 , -0.2770915 , -0.23759681],\n",
      "       [-0.49973986,  0.26079553, -0.34115666,  0.30289334, -0.36474857,\n",
      "         0.07131797, -0.18403384,  0.42260736, -0.1800819 , -0.2788125 ],\n",
      "       [-0.42066014, -0.27846357, -0.19009715, -0.32530874,  0.23028278,\n",
      "         0.24113011,  0.26671737,  0.5343131 , -0.31289423,  0.29959327]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(10, 3) dtype=float32, numpy=\n",
      "array([[-0.21633103,  0.53205967, -0.3140176 ],\n",
      "       [ 0.34449196,  0.12087828,  0.4463284 ],\n",
      "       [ 0.3826456 , -0.16180444,  0.4336028 ],\n",
      "       [ 0.27794254,  0.14037257,  0.30909556],\n",
      "       [ 0.07500541, -0.6051828 , -0.21971047],\n",
      "       [-0.16135514, -0.5013077 , -0.2064074 ],\n",
      "       [ 0.3575729 , -0.13655615, -0.6078898 ],\n",
      "       [-0.67033845, -0.15101182,  0.6227977 ],\n",
      "       [-0.07316196, -0.07758158, -0.6225958 ],\n",
      "       [ 0.5071472 , -0.5190166 ,  0.29440552]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and parse the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_training.csv', 'iris_test.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/lecture04/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parsing function\n",
    "def parse_single_example(record):\n",
    "    decoded = tf.decode_csv(record, [[.0],[.0],[.0],[.0],[]])\n",
    "    features = decoded[:4]\n",
    "    label = tf.cast(x = decoded[4], dtype = tf.int32)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = .03\n",
    "tr_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_training.csv')\n",
    "tr_dataset = tr_dataset.map(parse_single_example)\n",
    "tr_dataset = tr_dataset.shuffle(200).batch(batch_size = batch_size)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "global_step = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, ce_loss : 0.970\n",
      "epoch :   2, ce_loss : 0.805\n",
      "epoch :   3, ce_loss : 0.725\n",
      "epoch :   4, ce_loss : 0.657\n",
      "epoch :   5, ce_loss : 0.570\n",
      "epoch :   6, ce_loss : 0.494\n",
      "epoch :   7, ce_loss : 0.435\n",
      "epoch :   8, ce_loss : 0.391\n",
      "epoch :   9, ce_loss : 0.401\n",
      "epoch :  10, ce_loss : 0.388\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for mb_x, mb_y in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, mb_x, mb_y)\n",
    "        grads = tape.gradient(tr_loss, model.variables)\n",
    "        opt.apply_gradients(zip(grads, model.variables), global_step = global_step)\n",
    "        \n",
    "        avg_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_loss /= tr_step\n",
    "    \n",
    "    print('epoch : {:3}, ce_loss : {:.3f}'.format(epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_test.csv')\n",
    "tst_dataset = tst_dataset.map(parse_single_example)\n",
    "tst_dataset = tst_dataset.batch(batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_x, tst_y = next(iter(tst_dataset))\n",
    "tst_yhat = tf.argmax(model(tst_x), axis = -1, output_type = tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 96.67%\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy : {:.2%}'.format(np.mean(tf.equal(tst_y, tst_yhat))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
