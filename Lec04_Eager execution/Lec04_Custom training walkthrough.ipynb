{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 20 : TensorFlow for Deep Learning Research\n",
    "## Lecture 04 : Eager execution\n",
    "### Custon training walkthrough\n",
    "Categorizing Iris flowers by species by using Tensorflow's eager execution.\n",
    "\n",
    "This guide uses these high-level TensorFlow concepts:\n",
    "\n",
    "* Enable an [eager execution](https://www.tensorflow.org/guide/eager?hl=ko) development environment,\n",
    "* Import data with the [Datasets API](https://www.tensorflow.org/guide/datasets?hl=ko)\n",
    "* Build models and layers with TensorFlow's [Keras API](https://keras.io/getting-started/sequential-model-guide/)  \n",
    "  \n",
    "  \n",
    "* Reference\n",
    "    + https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape = (4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, features, label):\n",
    "    score = model(features)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels = label, logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 10) dtype=float32, numpy=\n",
      "array([[-0.30803984, -0.01893753,  0.61146855, -0.47435147,  0.09735668,\n",
      "        -0.40768373, -0.17765707, -0.5322173 ,  0.4472704 , -0.37332752],\n",
      "       [-0.49528438,  0.40341616,  0.55749846, -0.16490567,  0.38638103,\n",
      "        -0.43290883, -0.47683632,  0.6377454 ,  0.2413348 , -0.45055968],\n",
      "       [ 0.09948409,  0.32634228, -0.5646049 ,  0.34635472,  0.11541736,\n",
      "         0.64992034, -0.21107197, -0.61615974, -0.6445855 , -0.41054142],\n",
      "       [-0.51387227, -0.1673187 , -0.16908056, -0.25134146,  0.0717966 ,\n",
      "         0.2677107 ,  0.2694602 ,  0.35460413, -0.47340906, -0.46411705]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[-0.50144494, -0.47140843, -0.12475327,  0.4600966 ,  0.26963925,\n",
      "        -0.07112435, -0.37263134,  0.36020374,  0.40034693,  0.09232336],\n",
      "       [ 0.2787096 , -0.33051538, -0.13362941, -0.23161292, -0.30273294,\n",
      "        -0.25172636, -0.3334605 , -0.2700193 ,  0.42447084,  0.27119154],\n",
      "       [-0.5352003 , -0.11685625,  0.19772255,  0.16723603,  0.07714862,\n",
      "        -0.01583147, -0.20373645, -0.21856269,  0.03098613, -0.06459993],\n",
      "       [ 0.48325348, -0.44829515,  0.07423192,  0.49166155,  0.2007451 ,\n",
      "        -0.2784218 ,  0.1596911 ,  0.19561619, -0.03196567, -0.46072537],\n",
      "       [-0.3692549 ,  0.4758625 , -0.29366994,  0.5341115 , -0.47231105,\n",
      "        -0.11476567,  0.42504644, -0.35455573, -0.39965677,  0.41581565],\n",
      "       [ 0.39952302, -0.45729667,  0.01553845,  0.10801131, -0.3919884 ,\n",
      "        -0.13798475, -0.007514  ,  0.01269358, -0.02431613, -0.33584583],\n",
      "       [ 0.39282376,  0.40507615,  0.53226304, -0.29380184, -0.5462285 ,\n",
      "         0.50572157,  0.23964798, -0.06964073, -0.0509136 ,  0.37495017],\n",
      "       [ 0.03246397,  0.08226633, -0.33196634, -0.29088137, -0.21008718,\n",
      "        -0.36020675, -0.00201052,  0.05619532,  0.5055573 , -0.22660792],\n",
      "       [-0.45486996,  0.1160022 , -0.16193926, -0.04443151,  0.106897  ,\n",
      "         0.47887206,  0.13417917, -0.5313471 ,  0.00364065, -0.12287292],\n",
      "       [ 0.2668823 , -0.47094968,  0.3949867 , -0.21105719, -0.06430036,\n",
      "        -0.16635248, -0.28382602, -0.14521092, -0.23437366, -0.20552382]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(10, 3) dtype=float32, numpy=\n",
      "array([[-0.21521845,  0.6761384 ,  0.34856975],\n",
      "       [ 0.01474851, -0.25889897,  0.21357524],\n",
      "       [-0.47018242, -0.18735915,  0.2430821 ],\n",
      "       [-0.5693114 ,  0.23830354, -0.07906157],\n",
      "       [ 0.04628474,  0.35264504, -0.6780546 ],\n",
      "       [ 0.3652202 ,  0.11974895, -0.12941396],\n",
      "       [-0.41686934, -0.1267801 ,  0.1269871 ],\n",
      "       [ 0.5764396 , -0.27140707,  0.20222008],\n",
      "       [ 0.08279341,  0.43343902, -0.01707447],\n",
      "       [ 0.37915838,  0.06316966, -0.04033238]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and parse the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_training.csv', 'iris_test.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/lecture04/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parsing function\n",
    "def parse_single_example(record):\n",
    "    decoded = tf.decode_csv(record, [[.0],[.0],[.0],[.0],[]])\n",
    "    features = decoded[:4]\n",
    "    label = tf.cast(x = decoded[4], dtype = tf.int32)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = .03\n",
    "tr_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_training.csv')\n",
    "tr_dataset = tr_dataset.map(parse_single_example)\n",
    "tr_dataset = tr_dataset.shuffle(200).batch(batch_size = batch_size)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "global_step = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, ce_loss : 1.011\n",
      "epoch :   2, ce_loss : 0.726\n",
      "epoch :   3, ce_loss : 0.580\n",
      "epoch :   4, ce_loss : 0.490\n",
      "epoch :   5, ce_loss : 0.442\n",
      "epoch :   6, ce_loss : 0.408\n",
      "epoch :   7, ce_loss : 0.378\n",
      "epoch :   8, ce_loss : 0.335\n",
      "epoch :   9, ce_loss : 0.304\n",
      "epoch :  10, ce_loss : 0.297\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for mb_x, mb_y in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, mb_x, mb_y)\n",
    "        grads = tape.gradient(tr_loss, model.variables)\n",
    "        opt.apply_gradients(zip(grads, model.variables), global_step = global_step)\n",
    "        \n",
    "        avg_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_loss /= tr_step\n",
    "    \n",
    "    print('epoch : {:3}, ce_loss : {:.3f}'.format(epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_test.csv')\n",
    "tst_dataset = tst_dataset.map(parse_single_example)\n",
    "tst_dataset = tst_dataset.batch(batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_x, tst_y = next(iter(tst_dataset))\n",
    "tst_yhat = tf.argmax(model(tst_x), axis = -1, output_type = tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 80.00%\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy : {:.2%}'.format(np.mean(tf.equal(tst_y, tst_yhat))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
