{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 20 : TensorFlow for Deep Learning Research\n",
    "## Lecture 04 : Eager execution\n",
    "### Custon training walkthrough\n",
    "Categorizing Iris flowers by species by using Tensorflow's eager execution.\n",
    "\n",
    "This guide uses these high-level TensorFlow concepts:\n",
    "\n",
    "* Enable an [eager execution](https://www.tensorflow.org/guide/eager?hl=ko) development environment,\n",
    "* Import data with the [Datasets API](https://www.tensorflow.org/guide/datasets?hl=ko)\n",
    "* Build models and layers with TensorFlow's [Keras API](https://keras.io/getting-started/sequential-model-guide/)  \n",
    "  \n",
    "  \n",
    "* Reference\n",
    "    + https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu, input_shape = (4,)))\n",
    "model.add(tf.keras.layers.Dense(10, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, features, label):\n",
    "    score = model(features)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels = label, logits = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense/kernel:0' shape=(4, 10) dtype=float32, numpy=\n",
      "array([[ 0.23273689,  0.59694576,  0.53505063, -0.41671774,  0.5939325 ,\n",
      "         0.5985272 ,  0.16594094,  0.22990024,  0.24404621, -0.57850736],\n",
      "       [ 0.50012505,  0.28323126,  0.51059794, -0.34209886,  0.07642943,\n",
      "         0.42025232, -0.3534193 , -0.25960475,  0.3988968 ,  0.13867897],\n",
      "       [ 0.37427866,  0.11757517, -0.5074648 ,  0.27821964,  0.64113617,\n",
      "         0.54645467, -0.5339945 , -0.32484716,  0.5371665 ,  0.52713263],\n",
      "       [ 0.64209175,  0.27604747, -0.64053625, -0.14172429,  0.12444371,\n",
      "         0.14510643,  0.14198619,  0.34790993, -0.41857135, -0.6528216 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[-0.44230747,  0.31619924,  0.426068  , -0.10556999, -0.32908076,\n",
      "        -0.10463578,  0.43683833, -0.28880256,  0.38482857, -0.17079139],\n",
      "       [ 0.22581917,  0.12362629, -0.32890302, -0.49058792,  0.23691177,\n",
      "         0.17735404, -0.02634609, -0.01699305, -0.15909195, -0.02564263],\n",
      "       [ 0.3513354 , -0.26781693,  0.34309578,  0.2239694 ,  0.28953826,\n",
      "         0.4446996 , -0.49994892,  0.09023565, -0.03108525, -0.11956084],\n",
      "       [-0.1692037 , -0.20606902,  0.39730996, -0.38653314, -0.27865997,\n",
      "         0.10722429, -0.4018518 , -0.2735864 ,  0.4665265 , -0.2060323 ],\n",
      "       [-0.39374468,  0.49353147,  0.23548186,  0.04903078,  0.29438752,\n",
      "        -0.49697417,  0.2944702 ,  0.05209428, -0.29085094, -0.29972893],\n",
      "       [-0.51131815, -0.03142715, -0.22529471,  0.18120283,  0.03345394,\n",
      "        -0.24586353, -0.43438083, -0.16199583, -0.43990833, -0.03168988],\n",
      "       [-0.17431986, -0.22997367, -0.47305658, -0.17687142,  0.14594561,\n",
      "         0.02874815,  0.21374792,  0.45914435,  0.4402259 ,  0.15496904],\n",
      "       [-0.22790202,  0.50139475, -0.19545227,  0.19072568, -0.46344915,\n",
      "        -0.3825407 ,  0.29831547, -0.08024639,  0.38527375, -0.2816968 ],\n",
      "       [-0.4678477 , -0.24574703, -0.47316876, -0.18447655,  0.01979452,\n",
      "         0.54241705, -0.4287119 , -0.46860057,  0.01974988, -0.20535052],\n",
      "       [-0.05311754,  0.5265119 , -0.47543418,  0.24673885, -0.12262964,\n",
      "        -0.44495055, -0.10374165, -0.06392521, -0.4839738 , -0.4621049 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(10, 3) dtype=float32, numpy=\n",
      "array([[-0.60790807, -0.4530513 , -0.57850647],\n",
      "       [-0.5121423 ,  0.0045957 , -0.5627432 ],\n",
      "       [-0.07246083, -0.05761188, -0.00601554],\n",
      "       [-0.43483657, -0.3155324 , -0.38048992],\n",
      "       [ 0.64572656, -0.16747159, -0.06710464],\n",
      "       [-0.12165684,  0.19250602,  0.5441569 ],\n",
      "       [-0.03878701,  0.20354223,  0.3690554 ],\n",
      "       [-0.14515954,  0.2992273 , -0.17292422],\n",
      "       [-0.23854521,  0.21151978, -0.48470157],\n",
      "       [ 0.20653033,  0.21603364, -0.15056282]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and parse the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_training.csv', 'iris_test.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/lecture04/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parsing function\n",
    "def parse_single_example(record):\n",
    "    decoded = tf.decode_csv(record, [[.0],[.0],[.0],[.0],[]])\n",
    "    features = decoded[:4]\n",
    "    label = tf.cast(x = decoded[4], dtype = tf.int32)\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = .03\n",
    "tr_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_training.csv')\n",
    "tr_dataset = tr_dataset.map(parse_single_example)\n",
    "tr_dataset = tr_dataset.shuffle(200).batch(batch_size = batch_size)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "global_step = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, ce_loss : 0.997\n",
      "epoch :   2, ce_loss : 0.758\n",
      "epoch :   3, ce_loss : 0.621\n",
      "epoch :   4, ce_loss : 0.537\n",
      "epoch :   5, ce_loss : 0.493\n",
      "epoch :   6, ce_loss : 0.457\n",
      "epoch :   7, ce_loss : 0.427\n",
      "epoch :   8, ce_loss : 0.394\n",
      "epoch :   9, ce_loss : 0.368\n",
      "epoch :  10, ce_loss : 0.326\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    tr_step = 0\n",
    "    \n",
    "    for mb_x, mb_y in tr_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            tr_loss = loss_fn(model, mb_x, mb_y)\n",
    "        grads = tape.gradient(tr_loss, model.variables)\n",
    "        opt.apply_gradients(zip(grads, model.variables), global_step = global_step)\n",
    "        \n",
    "        avg_loss += tr_loss\n",
    "        tr_step += 1\n",
    "    else:\n",
    "        avg_loss /= tr_step\n",
    "    \n",
    "    print('epoch : {:3}, ce_loss : {:.3f}'.format(epoch + 1, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_dataset = tf.data.TextLineDataset(filenames = '../data/lecture04/iris_test.csv')\n",
    "tst_dataset = tst_dataset.map(parse_single_example)\n",
    "tst_dataset = tst_dataset.batch(batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_x, tst_y = next(iter(tst_dataset))\n",
    "tst_yhat = tf.argmax(model(tst_x), axis = -1, output_type = tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 93.33%\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy : {:.2%}'.format(np.mean(tf.equal(tst_y, tst_yhat))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
